include::{mod-loc}shared/all-attributes.adoc[]

[id="configuring-the-registry_{context}"]
= Configuring your {registry} deployment

[role="_abstract"]
This chapter explains how to set important configuration options for your {registry} deployment. This includes features such as the {registry} web console, logging, health checks, and observability:

* xref:configuring-registry-ui_{context}[]
* xref:configuring-registry-observability_{context}[]
* xref:configuring-liveness-readiness-probes_{context}[]
* xref:registry-liveness-env-vars_{context}[]

NOTE: For a list of all available configuration options, see {registry-config-reference}.

//INCLUDES



[id="configuring-registry-ui_{context}"]
== Configuring the {registry} web console

[role="_abstract"]
You can set optional environment variables to configure the {registry} web console specifically for your deployment environment or to customize its behavior.

.Prerequisites
* You have already installed {registry}.

[discrete]
=== Configuring the web console deployment environment

When you access the {registry} web console in your browser, some initial configuration settings are loaded. The following configuration settings are required:

* URL for core {registry} server REST API v3

Typically the {operator} will automatically configure the UI component with the REST API v3 URL.  However, you can override this value by configuring the appropriate environment variable in the UI component deployment configuration.

.Procedure
Configure the following environment variables to override the default URL:

* `REGISTRY_API_URL`: Specifies the URL for the core {registry} server REST API v3. For example, `\https://registry-api.my-domain.com/apis/registry/v3`

[discrete]
=== Configuring the web console in read-only mode

You can configure the {registry} web console in read-only mode as an optional feature. This mode disables all features in the {registry} web console that allow users to make changes to registered artifacts. For example, this includes the following:

* Creating a group
* Creating an artifact
* Uploading a new artifact version
* Updating artifact metadata
* Deleting an artifact

.Procedure
Configure the following environment variable:

* `REGISTRY_FEATURE_READ_ONLY`: Set to `true` to enable read-only mode. Defaults to `false`.



// Metadata created by nebel
// ParentAssemblies: assemblies/getting-started/as_installing-the-registry.adoc

[id="configuring-registry-observability_{context}"]
== Configuring {registry} observability with OpenTelemetry

[role="_abstract"]
You can configure {registry} to export telemetry data using OpenTelemetry (OTel) for comprehensive observability. This includes distributed tracing, metrics export via OTLP protocol, and log correlation with trace context.

{registry} is built with OpenTelemetry support, but individual telemetry signals (traces, metrics, logs) are disabled by default. When enabled, {registry} exports telemetry data to an OpenTelemetry-compatible collector such as Jaeger, Grafana Tempo, or the OpenTelemetry Collector.

.Prerequisites
* You have already installed {registry}.
* You have an OpenTelemetry-compatible backend available (for example, Jaeger, Grafana Tempo, or OpenTelemetry Collector).

[discrete]
=== Enabling OpenTelemetry

To enable OpenTelemetry observability, configure the following environment variables:

.Environment variables for enabling OpenTelemetry signals
[.table-expandable,width="100%",cols="4,6",options="header"]
|===
|Environment Variable
|Description
|`QUARKUS_OTEL_TRACES_ENABLED`
|Set to `true` to enable distributed tracing. Default is `false`.
|`QUARKUS_OTEL_METRICS_ENABLED`
|Set to `true` to enable metrics export via OTLP. Default is `false`.
|`QUARKUS_OTEL_LOGS_ENABLED`
|Set to `true` to enable log export via OTLP. Default is `false`.
|`QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT`
|The endpoint URL of your OpenTelemetry collector. For example, `http://jaeger:4317` for gRPC or `http://jaeger:4318` for HTTP.
|===

.Example: Enabling tracing with Jaeger
[source,yaml]
----
environment:
  QUARKUS_OTEL_TRACES_ENABLED: "true"
  QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT: "http://jaeger:4317"
----

.Example: Enabling all telemetry signals
[source,yaml]
----
environment:
  QUARKUS_OTEL_TRACES_ENABLED: "true"
  QUARKUS_OTEL_METRICS_ENABLED: "true"
  QUARKUS_OTEL_LOGS_ENABLED: "true"
  QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
----

[discrete]
=== Configuring trace sampling for production

In production environments, you should configure trace sampling to reduce overhead and control the volume of trace data:

.Environment variables for trace sampling
[.table-expandable,width="100%",cols="4,6",options="header"]
|===
|Environment Variable
|Description
|`QUARKUS_OTEL_TRACES_SAMPLER`
|The sampling strategy. Use `parentbased_traceidratio` for production.
|`QUARKUS_OTEL_TRACES_SAMPLER_ARG`
|The sampling ratio (0.0 to 1.0). A value of `0.1` samples 10% of traces.
|===

.Example: Production sampling configuration (10% of traces)
[source,yaml]
----
environment:
  QUARKUS_OTEL_TRACES_ENABLED: "true"
  QUARKUS_OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
  QUARKUS_OTEL_TRACES_SAMPLER: "parentbased_traceidratio"
  QUARKUS_OTEL_TRACES_SAMPLER_ARG: "0.1"
----

[discrete]
=== Configuring structured logging with trace context

When using JSON logging format, {registry} automatically includes trace context (trace ID and span ID) in log entries. This enables correlation between logs and traces.

.Example: Enabling structured logging with trace context
[source,yaml]
----
environment:
  QUARKUS_OTEL_TRACES_ENABLED: "true"
  QUARKUS_LOG_CONSOLE_JSON: "true"
----

[discrete]
=== OpenTelemetry features in {registry}

When OpenTelemetry is enabled, {registry} provides the following observability features:

* *Distributed tracing*: All REST API requests are automatically traced with spans containing request details, path parameters, and Apicurio-specific attributes such as `groupId`, `artifactId`, and `version`.

* *Storage layer tracing*: All storage operations create child spans, enabling you to trace the complete request flow from REST API to database.

* *Kafka tracing*: When using KafkaSQL storage, Kafka operations are automatically traced with context propagation.

* *Custom metrics*: OpenTelemetry metrics for artifact operations, schema validations, and search requests are exported alongside existing Prometheus metrics.

* *Log correlation*: When JSON logging is enabled, trace context is automatically injected into log entries for easy correlation.

[discrete]
=== Backwards compatibility

OpenTelemetry support is fully backwards compatible:

* The existing Prometheus metrics endpoint (`/q/metrics`) remains available and unchanged.
* Health check endpoints (`/q/health/*`) continue to work as before.
* All existing Micrometer-based metrics continue to function.

[role="_additional-resources"]
.Additional resources
* link:https://opentelemetry.io/docs/[OpenTelemetry Documentation]
* link:https://quarkus.io/guides/opentelemetry[Quarkus OpenTelemetry Guide]
* link:https://www.jaegertracing.io/docs/[Jaeger Documentation]


[id="configuring-liveness-readiness-probes_{context}"]

=== Configuring {registry} health checks on OpenShift

[role="_abstract"]
You can configure optional environment variables for liveness and readiness probes to monitor the health of the {registry} server on OpenShift:

* _Liveness probes_ test if the application can make progress. If the application cannot make progress, OpenShift automatically restarts the failing Pod.

* _Readiness probes_ test if the application is ready to process requests. If the application is not ready, it can become overwhelmed by requests, and OpenShift stops sending requests for the time that the probe fails. If other Pods are OK, they continue to receive requests.

IMPORTANT: The default values of the liveness and readiness environment variables are designed for most cases and should only be changed if required by your environment. Any changes to the defaults depend on your hardware, network, and amount of data stored. These values should be kept as low as possible to avoid unnecessary overhead.

.Prerequisites
* You must have an OpenShift cluster with cluster administrator access.
* You must have already installed {registry} on OpenShift.
* You must have already installed and configured your chosen {registry} storage in either {kafka-streams} or PostgreSQL.

.Procedure

. In the OpenShift Container Platform web console, log in using an account with cluster administrator privileges.

. Click *Installed Operators* > *{registry}*.

. On the *ApicurioRegistry* tab, click the Operator custom resource for your deployment, for example, *example-apicurioregistry*.

. In the main overview page, find the *Deployment Name* section and the corresponding `DeploymentConfig` name for your {registry} deployment, for example, *example-apicurioregistry*.

. In the left navigation menu, click *Workloads* > *Deployment Configs*, and select your `DeploymentConfig` name.

. Click the *Environment* tab, and enter your environment variables in the *Single values env* section, for example:
** *NAME*: `LIVENESS_STATUS_RESET`
** *VALUE*: `350`

. Click *Save* at the bottom.
+
Alternatively, you can perform these steps using the OpenShift `oc` command. For more details, see the link:https://docs.openshift.com/container-platform/{registry-ocp-version}/cli_reference/openshift_cli/getting-started-cli.html[OpenShift CLI documentation].

[role="_additional-resources"]
.Additional resources
* xref:registry-liveness-env-vars_{context}[]
* link:https://docs.openshift.com/container-platform/{registry-ocp-version}/applications/application-health.html[OpenShift documentation on monitoring application health]
//* TBD



// Metadata created by nebel
// ParentAssemblies: assemblies/getting-started/as_registry-reference.adoc

[id="registry-liveness-env-vars_{context}"]
== Environment variables for {registry} health checks

This section describes the available environment variables for {registry} health checks on OpenShift. These include liveness and readiness probes to monitor the health of the {registry} server on OpenShift. For an example procedure, see xref:configuring-liveness-readiness-probes_{context}[].

IMPORTANT: The following environment variables are provided for reference only. The default values are designed for most cases and should only be changed if required by your environment. Any changes to the defaults depend on your hardware, network, and amount of data stored. These values should be kept as low as possible to avoid unnecessary overhead.

[discrete]
=== Liveness environment variables

.Environment variables for {registry} liveness probes
//[%header,cols="5,5,2,5"]
[.table-expandable,width="100%",cols="5,5,2,5",options="header"]
|===
|Name
|Description
|Type
|Default
|`LIVENESS_ERROR_THRESHOLD`
|Number of liveness issues or errors that can occur before the liveness probe fails.
|Integer
|`1`
|`LIVENESS_COUNTER_RESET`
|Period in which the threshold number of errors must occur. For example, if this value is 60 and the threshold is 1, the check fails after two errors occur in 1 minute
|Seconds
|`60`
|`LIVENESS_STATUS_RESET`
|Number of seconds that must elapse without any more errors for the liveness probe to reset to OK status.
|Seconds
|`300`
|`LIVENESS_ERRORS_IGNORED`
|Comma-separated list of ignored liveness exceptions.
|String
|`io.grpc.StatusRuntimeException,org.apache.kafka.streams.errors.InvalidStateStoreException`
|===

NOTE: Because OpenShift automatically restarts a Pod that fails a liveness check, the liveness settings, unlike readiness settings, do not directly affect behavior of {registry} on OpenShift.

[discrete]
=== Readiness environment variables

.Environment variables for {registry} readiness probes
//[%header,cols="4,5,2,2"]
[.table-expandable,width="100%",cols="4,5,2,2",options="header"]
|===
|Name
|Description
|Type
|Default
|`READINESS_ERROR_THRESHOLD`
|Number of readiness issues or errors that can occur before the readiness probe fails.
|Integer
|`1`
|`READINESS_COUNTER_RESET`
|Period in which the threshold number of errors must occur. For example, if this value is 60 and the threshold is 1, the check fails after two errors occur in 1 minute.
|Seconds
|`60`
|`READINESS_STATUS_RESET`
|Number of seconds that must elapse without any more errors for the liveness probe to reset to OK status. In this case, this means how long the Pod stays not ready, until it returns to normal operation.
|Seconds
|`300`
|`READINESS_TIMEOUT`
a|Readiness tracks the timeout of two operations:

* How long it takes for storage requests to complete
* How long it takes for HTTP REST API requests to return a response

If these operations take more time than the configured timeout, this is counted as a readiness issue or error. This value controls the timeouts for both operations.
|Seconds
|`5`
|===


.Additional resources
* xref:configuring-liveness-readiness-probes_{context}[]
* link:https://docs.openshift.com/container-platform/{registry-ocp-version}/applications/application-health.html[OpenShift documentation on monitoring application health]
//* TBD


