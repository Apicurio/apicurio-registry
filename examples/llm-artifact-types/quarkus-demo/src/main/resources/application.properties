# =============================================================================
# Apicurio Registry LangChain4j Demo Configuration
# =============================================================================

# -----------------------------------------------------------------------------
# Apicurio Registry Connection
# -----------------------------------------------------------------------------
# URL of the Apicurio Registry instance
apicurio.registry.url=http://localhost:8080

# Default group for prompt templates (use "default" if not specified)
apicurio.registry.default-group=default

# Enable prompt template caching (recommended for production)
apicurio.registry.cache-enabled=true

# -----------------------------------------------------------------------------
# LLM Provider: Ollama (Default - Free, Local)
# -----------------------------------------------------------------------------
# Ollama runs locally and is completely free. No API key required.
# Install: brew install ollama && brew services start ollama && ollama pull llama3.2
quarkus.langchain4j.ollama.base-url=http://localhost:11434
quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.chat-model.temperature=0.7
quarkus.langchain4j.ollama.timeout=120s

# -----------------------------------------------------------------------------
# LLM Provider: OpenAI (Alternative - Paid API)
# -----------------------------------------------------------------------------
# To use OpenAI instead of Ollama:
# 1. Replace quarkus-langchain4j-ollama with quarkus-langchain4j-openai in pom.xml
# 2. Uncomment the lines below and comment out the Ollama config above
# 3. Set OPENAI_API_KEY environment variable
#
# quarkus.langchain4j.openai.api-key=${OPENAI_API_KEY}
# quarkus.langchain4j.openai.chat-model.model-name=gpt-4o
# quarkus.langchain4j.openai.chat-model.temperature=0.7

# -----------------------------------------------------------------------------
# Application Configuration
# -----------------------------------------------------------------------------
# Port 8081 to avoid conflict with Apicurio Registry on 8080
quarkus.http.port=8081

# -----------------------------------------------------------------------------
# RAG Configuration (Easy RAG with Ollama Embeddings)
# -----------------------------------------------------------------------------
# Use Ollama for embeddings (nomic-embed-text model)
# Install: ollama pull nomic-embed-text
quarkus.langchain4j.ollama.embedding-model.model-id=nomic-embed-text

# Easy RAG settings - we manage ingestion programmatically from web docs
# Path is required but not used since we fetch docs from web at startup
quarkus.langchain4j.easy-rag.path=.
quarkus.langchain4j.easy-rag.ingestion-strategy=off

# -----------------------------------------------------------------------------
# Logging
# -----------------------------------------------------------------------------
quarkus.log.category."io.apicurio".level=DEBUG
quarkus.log.category."dev.langchain4j".level=DEBUG
